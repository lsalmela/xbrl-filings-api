"""
Fetch mock URLs for tests and save them.

This module is a standalone script and it is not available for
importing.

The fetched URLs will be saved to YAML files in directory
`MOCK_URL_DIR_NAME` inside `tests` package.

.. note::
    This script uses beta feature `responses._recorder` (as of
    `responses` version 0.23.3).
"""

# SPDX-FileCopyrightText: 2023 Lauri Salmela <lauri.m.salmela@gmail.com>
#
# SPDX-License-Identifier: MIT

import argparse
from dataclasses import dataclass, field
from pathlib import Path
from types import FunctionType

import requests
from responses import _recorder

MOCK_URL_DIR_NAME = 'mock_responses'
CONFTEST_SRC_PATH = 'conftest_source.py'
CONFTEST_OUT_PATH = 'conftest.py'
entry_point_url = 'https://filings.xbrl.org/api/filings'

conftest_src_spath = str(Path(__file__).parent / CONFTEST_SRC_PATH)
conftest_out_spath = str(Path(__file__).parent / CONFTEST_OUT_PATH)
mock_dir_path = Path(__file__).parent / MOCK_URL_DIR_NAME
JSON_API_HEADERS = {
    'Content-Type': 'application/vnd.api+json'
    }
REQUEST_TIMEOUT = 30.0

URL_MOCK_FIXTURE_TEMPLATE = '''
@pytest.fixture
def {name}_response():
    """{docstring}"""
    with responses.RequestsMock({param_str}) as rsps:
        rsps._add_from_file(_get_path('{name}'))
        yield rsps
'''
URL_MOCK_PARAM_LAX = 'assert_all_requests_are_fired=False'
NO_EDIT_DOCSTRING = '''
DO NOT EDIT: This module is automatically generated by the script
``mock_upgrade.py``. Edit file ``conftest_source.py`` instead and run
aforementioned script.
'''


@dataclass
class _URLMock:
    name: str
    """Name of the URL mock collection."""
    lax_fixture: bool = field(kw_only=True, default=False)
    """
    Also create a fixture with a name ``<name>_lax``.

    The lax version of the fixture adds parameter
    ``assert_all_requests_are_fired=False`` to initiation of
    `responses.RequestsMock`. These fixtures are used when the test
    function should throw and not necessarily initiate a `requests`
    URL request.
    """


def _get_path(mock_name):
    """Get absolute file path of the mock URL collection file."""
    file_path = mock_dir_path / f'{mock_name}.yaml'
    return str(file_path)


###################### DEFINE MOCK URL COLLECTIONS #####################


@_recorder.record(file_path=_get_path('creditsuisse21en_by_id'))
def _fetch_creditsuisse21en_by_id():
    """Credit Suisse 2021 English AFR filing by `api_id`."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 1,
            # id = api_id
            'filter[id]': '162',
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('creditsuisse21en_by_id')


@_recorder.record(file_path=_get_path('asml22en'))
def _fetch_asml22en():
    """ASML Holding 2022 English AFR filing."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 1,
            # fxo_id = filing_index
            'filter[fxo_id]': '724500Y6DUVHQD6OXN27-2022-12-31-ESEF-NL-0',
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('asml22en')


@_recorder.record(file_path=_get_path('asml22en_entities'))
def _fetch_asml22en_entities():
    """ASML Holding 2022 English AFR filing with entity."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 1,
            # fxo_id = filing_index
            'filter[fxo_id]': '724500Y6DUVHQD6OXN27-2022-12-31-ESEF-NL-0',
            'include': 'entity'
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('asml22en_entities')


@_recorder.record(file_path=_get_path('asml22en_vmessages'))
def _fetch_asml22en_vmessages():
    """ASML Holding 2022 English AFR filing with validation messages."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 1,
            'include': 'validation_messages',
            # fxo_id = filing_index
            'filter[fxo_id]': '724500Y6DUVHQD6OXN27-2022-12-31-ESEF-NL-0'
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('asml22en_vmessages')


@_recorder.record(file_path=_get_path('asml22en_ent_vmsg'))
def _fetch_asml22en_ent_vmsg():
    """ASML Holding 2022 English AFR filing with entities and v-messages."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 1,
            # fxo_id = filing_index
            'filter[fxo_id]': '724500Y6DUVHQD6OXN27-2022-12-31-ESEF-NL-0',
            'include': 'entity,validation_messages'
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('asml22en_ent_vmsg')


@_recorder.record(file_path=_get_path('filter_language'))
def _fetch_filter_language():
    """Filter by language 'fi'."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 1,
            'filter[language]': 'fi',
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('filter_language')


@_recorder.record(file_path=_get_path('filter_last_end_date'))
def _fetch_filter_last_end_date():
    """Filter by last_end_date '2021-02-28'."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 1,
            'filter[period_end]': '2021-02-28', # last_end_date
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('filter_last_end_date', lax_fixture=True)


@_recorder.record(file_path=_get_path('filter_error_count'))
def _fetch_filter_error_count():
    """Filter by error_count value 1."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 1,
            'filter[error_count]': 1
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('filter_error_count')


@_recorder.record(file_path=_get_path('filter_added_time'))
def _fetch_filter_added_time():
    """Filter by added_time value '2021-09-23 00:00:00'."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 1,
            'filter[date_added]': '2021-09-23 00:00:00' # added_time
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('filter_added_time', lax_fixture=True)


@_recorder.record(file_path=_get_path('filter_entity_api_id'))
def _fetch_filter_entity_api_id():
    """Return error when filtering with `entity_api_id`."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 1,
            'filter[entity_api_id]': 1/0
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('filter_entity_api_id', lax_fixture=True)


@_recorder.record(file_path=_get_path('filter_package_url'))
def _fetch_filter_package_url():
    """Filter by package_url of Kone 2022 filing."""
    filter_url = (
        '/2138001CNF45JP5XZK38/2022-12-31/ESEF/FI/0/'
        '2138001CNF45JP5XZK38-2022-12-31-EN.zip'
        )
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 1,
            'filter[package_url]': filter_url
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('filter_package_url')


@_recorder.record(file_path=_get_path('filter_package_sha256'))
def _fetch_filter_package_sha256():
    """Filter by package_sha256 of Kone 2022 filing."""
    filter_sha = (
        'e489a512976f55792c31026457e86c9176d258431f9ed645451caff9e4ef5f80')
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 1,
            'filter[sha256]': filter_sha # package_sha256
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('filter_package_sha256')


@_recorder.record(file_path=_get_path('finnish_jan22'))
def _fetch_finnish_jan22():
    """Finnish AFR filings with reporting period ending in Jan 2022."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 2,
            'filter[country]': 'FI',
            'filter[period_end]': '2022-01-31' # last_end_date
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('finnish_jan22')


@_recorder.record(file_path=_get_path('oldest3_fi'))
def _fetch_oldest3_fi():
    """Oldest 3 AFR filings reported in Finland."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 3,
            'filter[country]': 'FI',
            'sort': 'date_added' # added_time
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('oldest3_fi')


@_recorder.record(file_path=_get_path('sort_two_fields'))
def _fetch_sort_two_fields():
    """
    Sort Finnish filings by `last_end_date` and `added_time`.

    .. warning::

        Volatile with ``mock_upgrade.py`` run. See test
        ``test_query::Test_get_filings::test_sort_two_fields``.
    """
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 2,
            'filter[country]': 'FI',
            'sort': 'period_end,processed' # last_end_date, processed_time
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('sort_two_fields')


@_recorder.record(file_path=_get_path('multipage'))
def _fetch_multipage():
    """Get 3 pages (2pc) of oldest Swedish filings."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 2,
            'filter[country]': 'SE',
            'sort': 'date_added' # added_time
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 2,
            'filter[country]': 'SE',
            'sort': 'date_added', # added_time
            'page[number]': 2
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 2,
            'filter[country]': 'SE',
            'sort': 'date_added', # added_time
            'page[number]': 3
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('multipage', lax_fixture=True)


@_recorder.record(file_path=_get_path('api_id_multifilter'))
def _fetch_api_id_multifilter():
    """Get 4 Shell filings for 2021 and 2022."""
    for id_i, api_id in enumerate(('1134', '1135', '4496', '4529')):
        _ = requests.get(
            url=entry_point_url,
            params={
                'page[size]': 4 - id_i,
                'filter[id]': api_id
                },
            headers=JSON_API_HEADERS,
            timeout=REQUEST_TIMEOUT
            )
    return _URLMock('api_id_multifilter')


@_recorder.record(file_path=_get_path('country_multifilter'))
def _fetch_country_multifilter():
    """Get three filings for the first country `FI`."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 3,
            'filter[country]': 'FI'
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('country_multifilter')


@_recorder.record(file_path=_get_path('filing_index_multifilter'))
def _fetch_filing_index_multifilter():
    """Get three filings for the first country `FI`."""
    fxo_codes = (
        '21380068P1DRHMJ8KU70-2021-12-31-ESEF-GB-0',
        '21380068P1DRHMJ8KU70-2021-12-31-ESEF-NL-0'
        )
    for fxo_i, fxo in enumerate(fxo_codes):
        _ = requests.get(
            url=entry_point_url,
            params={
                'page[size]': 2 - fxo_i,
                'filter[fxo_id]': fxo # filing_index
                },
            headers=JSON_API_HEADERS,
            timeout=REQUEST_TIMEOUT
            )
    return _URLMock('filing_index_multifilter')


@_recorder.record(file_path=_get_path('reporting_date_multifilter'))
def _fetch_reporting_date_multifilter():
    """Return an error for filtering with `reporting_date`."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 3,
            'filter[reporting_date]': '2020-12-31'
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('reporting_date_multifilter')


@_recorder.record(file_path=_get_path('inconsistency_count_multifilter'))
def _fetch_inconsistency_count_multifilter():
    """Return an error for filtering with `inconsistency_count`."""
    _ = requests.get(
        url=entry_point_url,
        params={
            'page[size]': 2,
            'filter[inconsistency_count]': 1
            },
        headers=JSON_API_HEADERS,
        timeout=REQUEST_TIMEOUT
        )
    return _URLMock('inconsistency_count_multifilter')


################ END OF MOCK URL COLLECTION DEFINITIONS ################


def main():
    """Run the command line interface."""
    parser = argparse.ArgumentParser(
        description=(
            'Script for updating mock URL collections for tests in '
            f'folder "{MOCK_URL_DIR_NAME}".'
            ),
        epilog=(
            'If no flags are given, default behavior is to upgrade all '
            'mock URL collections.'
            )
        )

    parser.add_argument(
        '-l', '--list', action='store_true',
        help='list all mocks defined in this module'
        )
    parser.add_argument(
        '-b', '--bare-list', action='store_true',
        help='use simple bare list format with --list'
        )
    parser.add_argument(
        '-n', '--new', action='store_true',
        help='upgrade only new, unfetched mock URL collections'
        )

    clargs = parser.parse_args()

    if clargs.list:
        _list_mock_urls(clargs.bare_list)
    elif clargs.new:
        _upgrade_mock_urls(only_new=True)
    else:
        _upgrade_mock_urls(only_new=False)


def _upgrade_mock_urls(only_new):
    # Ensure directory exists
    mock_dir_path.mkdir(parents=True, exist_ok=True)

    func_dict = _get_fetch_functions()
    fetch_mocks = None
    if only_new:
        fetch_mocks = _get_unfetched_mocks(func_dict)
        print(
            f'\nUpgrading {len(fetch_mocks)} unfetched mock URL '
            'collection(s)\n'
            )
    else:
        fetch_mocks = func_dict.keys()
        print(
            f'\nUpgrading all {len(func_dict)} mock URL collections\n')

    # Run recorder functions
    with open(conftest_out_spath, 'w', encoding='utf-8') as ctout:

        # Write non-generated conftest.py contents
        with open(conftest_src_spath, 'r', encoding='utf-8') as ctsource:
            skip_until_newline = False
            for line in ctsource:
                if skip_until_newline:
                    skip_until_newline = line != '\n'
                elif line.startswith('EDITABLE: '):
                    ctout.write(NO_EDIT_DOCSTRING.lstrip())
                    skip_until_newline = True
                else:
                    ctout.write(line)

        # Iterate URL mock collections, download and save request
        # contents and append conftext.py accordingly
        for mock_name in fetch_mocks:
            print(f'> {mock_name}')
            mock_func = func_dict[mock_name]

            # Fetch mock request content from the API and save it
            urlmock = mock_func()

            py_code = _mock_url_to_py_code(urlmock, mock_func.__doc__)
            ctout.write('\n' + py_code)

    _delete_files_of_removed_mocks(func_dict)

    if only_new:
        print(
            f'\nFetched {len(fetch_mocks)} new mock(s) '
            'of total {len(func_dict)}'
            )
    else:
        print('\nAll mocks upgraded')
    print(f'\nFolder path: {mock_dir_path}')


def _mock_url_to_py_code(urlmock, docstring):
    """Write generated conftest.py contents for URL mock collections."""
    gen_py_list = []
    for islax in range(2 if urlmock.lax_fixture else 1):
        mockname = urlmock.name
        param_str = ''
        if islax:
            mockname = f'{mockname}_lax'
            param_str = URL_MOCK_PARAM_LAX
        gen_py_list.append(
            URL_MOCK_FIXTURE_TEMPLATE.format(
                name=mockname,
                docstring=docstring,
                param_str=param_str
                ))
    return '\n'.join(gen_py_list)

def _list_mock_urls(bare_list):
    func_dict = _get_fetch_functions()
    if not bare_list:
        print(f'\nFound {len(func_dict)} mock URL collections:')
    new_mocks = _get_unfetched_mocks(func_dict)
    for mock_name in func_dict.keys():
        if bare_list:
            print(mock_name)
        else:
            print('\n' + mock_name, end='')
            if mock_name in new_mocks:
                print(' (unfetched)')
            else:
                print()
            print(f'    {func_dict[mock_name].__doc__.strip()}')


def _delete_files_of_removed_mocks(func_dict):
    mock_names = set(func_dict.keys())
    deleted_files = []
    for filepath in mock_dir_path.iterdir():
        if filepath.stem not in mock_names:
            filepath.unlink()
            deleted_files.append(filepath.name)
    if deleted_files:
        print('\nDeleted files of removed mocks in following files:\n')
        for filename in deleted_files:
            print(f'{MOCK_URL_DIR_NAME}/{filename}')


def _get_fetch_functions():
    """Get a dictionary of mock_name: function."""
    func_dict = {}
    gitems = globals().copy()
    for name, val in gitems.items():
        if name.startswith('_fetch_') and isinstance(val, FunctionType):
            func_dict[name[7:]] = val
    return func_dict


def _get_unfetched_mocks(func_dict):
    new_mocks = []
    for mock_name in func_dict.keys():
        mock_path = mock_dir_path / f'{mock_name}.yaml'
        if not mock_path.is_file():
            new_mocks.append(mock_name)
    return new_mocks


if __name__ == '__main__':
    main()
else:
    msg = 'This module must be run as a script.'
    raise NotImplementedError(msg)
